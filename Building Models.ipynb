{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import animation, rc\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import plot, init_notebook_mode\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import seaborn as sns\n",
    "from pyquaternion import Quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from pathlib import Path\n",
    "from keras.utils import to_categorical\n",
    "import struct\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import reduce\n",
    "from typing import Tuple, List, Dict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/media/brendanliu/1ffe4965-0a76-4845-aedc-1929d41a1cde/lyft_vis/3d-object-detection-for-autonomous-vehicles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22680/22680 [00:01<00:00, 20868.91it/s]\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(input_path+'train.csv')\n",
    "sample_submission = pd.read_csv(input_path+'sample_submission.csv')\n",
    "#Using the kaggle challenge description of the data.\n",
    "column_names = ['sample_token', 'object_id', 'center_x', 'center_y',\n",
    "                    'center_z', 'width', 'length', 'height', 'yaw','class_name']\n",
    "objects = []\n",
    "for sample_id, values in tqdm(train_csv.values[:]):\n",
    "    data_params = values.split()\n",
    "    num_obj = len(data_params)\n",
    "    for i in range(num_obj // 8):\n",
    "        x, y, z, w, l, h, yaw, c = tuple(data_params[i * 8: (i + 1) * 8])\n",
    "        objects.append([sample_id,i,x,y,z,w,l,h,yaw,c])\n",
    "train_data = pd.DataFrame(objects,columns=column_names)\n",
    "\n",
    "numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', \n",
    "                    'height', 'yaw']\n",
    "train_data[numerical_cols] = np.float32(train_data[numerical_cols].values)\n",
    "train_data.to_csv(input_path+'train_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_scene(index):\n",
    "    my_scene = lyft_dataset.scene[index]\n",
    "    my_sample_token = my_scene[\"first_sample_token\"]\n",
    "    lyft_dataset.render_sample(my_sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(input_path + 'train_dataframe.csv')\n",
    "testdf = pd.read_csv(input_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 10.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "shape = (100, 100, 3)\n",
    "MAX_VALUE = 140\n",
    "\n",
    "train_data = pd.read_csv(input_path + 'train.csv')\n",
    "\n",
    "lyft_data = LyftDataset(data_path = input_path, json_path = input_path + 'train_data')\n",
    "\n",
    "categories = [i['name'] for i in lyft_data.category]\n",
    "\n",
    "columns = ['confidence' ,'center_x', 'center_y', \"center_z\", 'width', 'length', 'height', 'rotate_w', 'rotate_x', 'rotate_y', 'rotate_z', 'class']\n",
    "sensors = lyft_data.sensor\n",
    "sensors = [i['channel'] for i in sensors]\n",
    "sensors = [i for i in sensors if 'LIDAR' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageFileNames(token : str):\n",
    "    \n",
    "    list_of_filenames = []\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        filename = lyft_data.get('sample_data', lyft_data.get('sample', token)['data'][sensor])['filename']\n",
    "        filename = input_path + 'train_images' + filename[6:]\n",
    "        list_of_filenames.append(filename)\n",
    "        \n",
    "    return list_of_filenames \n",
    "\n",
    "def getData(token):\n",
    "    \n",
    "    list_of_values = []\n",
    "    list_of_anns = lyft_data.get('sample', token)['anns']\n",
    "    for annotation_token in list_of_anns:\n",
    "        sample_data = lyft_data.get('sample_annotation', annotation_token)\n",
    "        list_of_values.append(sample_data['category_name'])\n",
    "    \n",
    "    return np.array(list_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/638179 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-958025c33109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mallfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImageFileNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mallfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def fit_model(model):\n",
    "    for token in tqdm(traindf['sample_token']):\n",
    "        allfiles = []\n",
    "        values = getData(token)\n",
    "        values = values.reshape((values.shape[0],) + (1, ) + (values.shape[1], ))\n",
    "        filenames = getImageFileNames(token)\n",
    "        allfiles.append(filenames)\n",
    "        images = [np.asarray(Image.open(i).resize(shape[:-1])).reshape(shape) for i in filenames]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
